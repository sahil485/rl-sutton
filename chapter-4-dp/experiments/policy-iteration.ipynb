{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f66a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/sahil/Projects/rl-sutton-exercises/venv/lib/python3.13/site-packages (2.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b0aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000b162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grid(grid: np.ndarray):\n",
    "    counter = 1\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            if (i == 0 and j == 0) or (i == 3 and j == 3):\n",
    "                grid[i, j] = 0\n",
    "            else:\n",
    "                grid[i, j] = counter\n",
    "                counter += 1               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a2c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_next_state(state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "    i, j = state\n",
    "    \n",
    "    if action == 'up':\n",
    "        next_i, next_j = max(0, i - 1), j\n",
    "    elif action == 'down':\n",
    "        next_i, next_j = min(3, i + 1), j\n",
    "    elif action == 'left':\n",
    "        next_i, next_j = i, max(0, j - 1)\n",
    "    elif action == 'right':\n",
    "        next_i, next_j = i, min(3, j + 1)\n",
    "    \n",
    "    return (next_i, next_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9709ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state_value_func(): \n",
    "    return np.zeros((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2e41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy():\n",
    "    return np.ones((4, 4, 4)) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b233df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def action_to_index(action: str, actions: List[str] = ['up', 'down', 'left', 'right']):\n",
    "    return actions.index(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5983209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def calculate_p(current_state: Tuple[int], action, next_state: Tuple[int], reward: int, adjacency: Dict[Tuple[int, int], List[Tuple[int, int]]]):\n",
    "    if next_state in adjacency[current_state] and reward == -1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1e83271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_improvement(v, policy, grid, actions, discount_factor):\n",
    "    # optimal policy is the one that maximizes the expected state-value-action q(s,a)\n",
    "    # argmax_a r(s, a) + gamma * v(s')\n",
    "    policy_stable = True\n",
    "    optimal_policy = policy.copy()\n",
    "    for state in [(i, j) for i in range(len(grid)) for j in range(len(grid[0])) if not (i ==0 and j == 0) and not (i == 3 and j == 3)]: # loop over all non-terminal states\n",
    "        init_best_action = policy[state[0], state[1], :].argmax()\n",
    "        best_action = actions[0]\n",
    "        best_state_action_value = None\n",
    "        curr_state_value = 0\n",
    "        for action in actions:\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = -1\n",
    "            curr_state_value = (reward + discount_factor * v[next_state])\n",
    "            if best_state_action_value is None or curr_state_value > best_state_action_value:\n",
    "                best_state_action_value = curr_state_value\n",
    "                best_action = action\n",
    "        if init_best_action != action_to_index(best_action):\n",
    "            policy_stable = False\n",
    "\n",
    "        optimal_policy[state[0], state[1], :] = 0\n",
    "        optimal_policy[state[0], state[1], action_to_index(best_action)] = 1\n",
    "\n",
    "    return optimal_policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4194f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA = 1e-10\n",
    "DISCOUNT_FACTOR = 1\n",
    "\n",
    "v = init_state_value_func()\n",
    "\n",
    "grid = np.zeros((4, 4))\n",
    "init_grid(grid)\n",
    "\n",
    "pi_a_given_s = 1/4 # pi(a | s)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "policy = init_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e94617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_evaluation(v, policy, grid, actions, discount_factor, theta):\n",
    "    error = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    while error > theta:\n",
    "        v_new = v.copy()\n",
    "        for state in [(i, j) for i in range(len(grid)) for j in range(len(grid[0])) if not (i ==0 and j == 0) and not (i == 3 and j == 3)]: # loop over all non-terminal states\n",
    "            curr_state_value = 0\n",
    "            for action in actions:\n",
    "                next_state = get_next_state(state, action)\n",
    "                reward = -1\n",
    "                curr_state_value += policy[state[0], state[1], action_to_index(action)] * (reward + discount_factor * v[next_state])\n",
    "\n",
    "            v_new[state[0], state[1]] = curr_state_value\n",
    "        error = np.max(np.abs(v_new - v))\n",
    "        v = v_new\n",
    "        counter += 1\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f5fc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_iteration(v_init, policy_init, grid, actions, discount_factor, theta):\n",
    "    policy_stable = False\n",
    "\n",
    "    v = v_init.copy()\n",
    "    policy = policy_init.copy()\n",
    "\n",
    "    while not policy_stable:\n",
    "        # Iterative Policy Evaluation\n",
    "        v = run_policy_evaluation(v, policy, grid, actions, discount_factor, theta)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy, is_policy_stable = run_policy_improvement(v, policy, grid, actions, discount_factor)\n",
    "        \n",
    "        if is_policy_stable:\n",
    "            policy_stable = True\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return v, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e22624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Optimal Policy:\n",
      "[[[0.25 0.25 0.25 0.25]\n",
      "  [0.   0.   1.   0.  ]\n",
      "  [0.   0.   1.   0.  ]\n",
      "  [0.   1.   0.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   1.   0.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   1.   0.   0.  ]\n",
      "  [0.   1.   0.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [0.   0.   0.   1.  ]\n",
      "  [0.   0.   0.   1.  ]\n",
      "  [0.25 0.25 0.25 0.25]]]\n"
     ]
    }
   ],
   "source": [
    "THETA = 1e-10\n",
    "DISCOUNT_FACTOR = 1\n",
    "\n",
    "v = init_state_value_func()\n",
    "\n",
    "grid = np.zeros((4, 4))\n",
    "init_grid(grid)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "policy = init_policy()\n",
    "\n",
    "v_optimal, policy_optimal = run_policy_iteration(v_init=v, policy_init=policy, grid=grid, actions=actions, discount_factor=DISCOUNT_FACTOR, theta=THETA)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "print(v_optimal)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bfe49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
