{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86e2e47",
   "metadata": {},
   "source": [
    "$\\text{Chapter 3 Exercises}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5203d",
   "metadata": {},
   "source": [
    "$\\textbf{3.4: Calculate Transition Probabilities}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$ \n",
    "\n",
    "Give a table analogous to that in Example 3.3, but for $p(s', r | s, a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s', r | s, a)$, and a row for every 4-tuple for which $p(s', r | s, a) > 0$.\n",
    "\n",
    "| $s$ | $a$ | $s'$ | $p(s'\\mid s, a)$ | $r(s, a, s')$ |\n",
    "|-----|-----|------|----------------------|---------------|\n",
    "| high | search | high | $\\alpha$ | $r_{\\text{search}}$ |\n",
    "| high | search | low | $1 - \\alpha$ | $r_{\\text{search}}$ |\n",
    "| low | search | high | $1 - \\beta$ | $-3$ |\n",
    "| low | search | low | $\\beta$ | $r_{\\text{search}}$ |\n",
    "| high | wait | high | $1$ | $r_{\\text{wait}}$ |\n",
    "| high | wait | low | $0$ | $-$ |\n",
    "| low | wait | high | $0$ | $-$ |\n",
    "| low | wait | low | $1$ | $r_{\\text{wait}}$ |\n",
    "| low | recharge | high | $1$ | $0$ |\n",
    "| low | recharge | low | $0$ | $-$ |\n",
    "\n",
    "$\\underline{\\text{Answer:}}$\n",
    "\n",
    "Since the reward is deterministic based on $s', a,$ and $s$, we have $p(s', r) = p(s')$. Thus, the rewards are\n",
    "\n",
    "| $s$ | $a$ | $s'$ | $r(s, a, s')$ | $p(s', r \\mid s, a)$ |\n",
    "|-----|-----|------|---------------|----------------------|\n",
    "| high | search | high | $r_{\\text{search}}$ | $\\alpha$ |\n",
    "| high | search | low | $r_{\\text{search}}$ | $1 - \\alpha$ |\n",
    "| low | search | high | $-3$ | $1 - \\beta$ |\n",
    "| low | search | low | $r_{\\text{search}}$ | $\\beta$ |\n",
    "| high | wait | high | $r_{\\text{wait}}$ | $1$ |\n",
    "| low | wait | low | $r_{\\text{wait}}$ | $1$ |\n",
    "| low | recharge | high | $0$ | $1$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9033774",
   "metadata": {},
   "source": [
    "$\\textbf{3.5: Probability Derivation for Episodic Tasks}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$\n",
    "\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of \n",
    "$$\\sum_{s'\\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', a|s, a) = 1, \\text{ for all } s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A}(s).$$\n",
    "\n",
    "$\\underline{\\text{Answer:}}$\n",
    "\n",
    "We need to include terminal states in the possible values for $s'$, meaning that the sum becomes\n",
    "\n",
    "$$\\sum_{s'\\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', a|s, a) = 1, \\text{ for all } s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A}(s).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731fbb1",
   "metadata": {},
   "source": [
    "$\\textbf{3.6: Episodic Pole Balancing w/ Discounting}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$\n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "$\\underline{\\text{Answer:}}$\n",
    "\n",
    "In this scenario, the return at each time would be\n",
    "\n",
    "$$G_t = -\\gamma^{T-t}.$$\n",
    "\n",
    "Assuming $T$ signals a terminal state, the larger $T$ is, the more maximal $G_t$ will be, meaning that again, we are rewarding longer sequences more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e29aad",
   "metadata": {},
   "source": [
    "$\\textbf{3.7: Episodic Escape from Maze}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$\n",
    "\n",
    "You have not effectively communicated the desired outcome to the agent because in any case, the maximal reward of any episode can only be $+1$. Therefore, the agent is not actually incentivized to escape from the maze quickly (or at all!), since escaping at $t=0$ yields the same reward as escaping at $t = \\infty$.\n",
    "\n",
    "A better reward system would be $-1$ for each timestep where the robot hasn't escaped. This way, as more time steps elapse, the reward gets smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117827f7",
   "metadata": {},
   "source": [
    "$\\textbf{3.9: Return Calculation}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$\n",
    "\n",
    "Suppose $\\gamma = 0.9$ and the reward sequence is $R_1= 2$ followed by an infinite sequence of $7$. What are $G_1$ and $G_0$.\n",
    "\n",
    "$\\underline{\\text{Answer:}}$\n",
    "\n",
    "$$G_0 = R_1 + \\gamma R_2 \\cdots = 2 + \\frac{7(0.9)}{1 - 0.9} = 65.$$\n",
    "\n",
    "$$G_1 = R_2 + \\gamma R_3 + \\cdots = \\frac{7}{1 - 0.9} = 70.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f52ac6",
   "metadata": {},
   "source": [
    "$\\textbf{3.9}$\n",
    "\n",
    "$\\underline{\\text{Question:}}$\n",
    "\n",
    "Given an equation for $v_{\\pi}$ in terms of $q_{\\pi}$ and $\\pi$.\n",
    "\n",
    "$\\underline{\\text{Answer:}}$\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\cdot q_\\pi(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dac63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
